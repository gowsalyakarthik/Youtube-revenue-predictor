{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d379590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/WELCOME/Desktop/project 3/youtube_ad_revenue_dataset.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows, Cols:\", df.shape)\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nNull % per column:\")\n",
    "print((df.isna().mean() * 100).round(2).sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nDuplicate rows %:\")\n",
    "print(round(df.duplicated().mean() * 100, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5acccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement rate: how reactive the audience is\n",
    "df[\"engagement_rate\"] = (df[\"likes\"] + df[\"comments\"]) / df[\"views\"]\n",
    "\n",
    "# Clean infinities from division by zero (safety)\n",
    "df[\"engagement_rate\"] = df[\"engagement_rate\"].replace([np.inf, -np.inf], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14256b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    \"views\",\n",
    "    \"likes\",\n",
    "    \"comments\",\n",
    "    \"watch_time_minutes\",\n",
    "    \"video_length_minutes\",\n",
    "    \"subscribers\",\n",
    "    \"engagement_rate\",\n",
    "    \"ad_revenue_usd\"\n",
    "]\n",
    "\n",
    "df[numeric_cols].describe(percentiles=[0.5, 0.9, 0.95, 0.99]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aefbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"views\"], df[\"ad_revenue_usd\"], alpha=0.3)\n",
    "plt.xlabel(\"views\")\n",
    "plt.ylabel(\"ad_revenue_usd\")\n",
    "plt.title(\"Views vs Revenue\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df[\"watch_time_minutes\"], df[\"ad_revenue_usd\"], alpha=0.3)\n",
    "plt.xlabel(\"watch_time_minutes\")\n",
    "plt.ylabel(\"ad_revenue_usd\")\n",
    "plt.title(\"Watch Time vs Revenue\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df[\"engagement_rate\"], df[\"ad_revenue_usd\"], alpha=0.3)\n",
    "plt.xlabel(\"engagement_rate\")\n",
    "plt.ylabel(\"ad_revenue_usd\")\n",
    "plt.title(\"Engagement Rate vs Revenue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# We will NOT impute missing values here.\n",
    "# We'll do imputation in the modeling pipeline.\n",
    "# But we DO save this cleaned version.\n",
    "\n",
    "df.to_csv(\"C:/Users/WELCOME/Desktop/project 3/youtube_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942dda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, pkgutil\n",
    "if pkgutil.find_loader(\"sklearn\") is None:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"joblib\", \"pandas\", \"numpy\"])\n",
    "import sklearn, joblib\n",
    "print(\"sklearn:\", sklearn.__version__, \"| joblib:\", joblib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "RAW_PATH = \"../data/youtube_raw.csv\"           # put your CSV here (rename from the original)\n",
    "CLEAN_PATH = \"../data/youtube_clean.csv\"\n",
    "APP_DIR = \"../app\"\n",
    "MODEL_PATH = os.path.join(APP_DIR, \"model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d57379",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_PATH = \"C:/Users/WELCOME/Desktop/project 3/youtube_clean.csv\"\n",
    "\n",
    "\n",
    "# 1) Load the cleaned snapshot from Step 1\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "\n",
    "# 2) Define feature groups and target\n",
    "numeric_cols = [\n",
    "    \"views\",\n",
    "    \"likes\",\n",
    "    \"comments\",\n",
    "    \"watch_time_minutes\",\n",
    "    \"video_length_minutes\",\n",
    "    \"subscribers\",\n",
    "    \"engagement_rate\",\n",
    "]\n",
    "\n",
    "categorical_cols = [\"category\", \"device\", \"country\"]\n",
    "target_col = \"ad_revenue_usd\"\n",
    "\n",
    "# 3) Impute missing values\n",
    "#    - numeric → median (robust to outliers)\n",
    "#    - categorical → mode (most frequent)\n",
    "for c in numeric_cols:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "for c in categorical_cols:\n",
    "    df[c] = df[c].fillna(df[c].mode()[0])\n",
    "\n",
    "# 4) Basic sanity checks\n",
    "null_pct_after = (df.isna().mean() * 100).round(2).sort_values(ascending=False)\n",
    "print(\"=== Imputation done ===\")\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "print(\"\\nAny remaining NaNs (top 10):\")\n",
    "print(null_pct_after.head(10))\n",
    "\n",
    "# 5) Quick peek to confirm columns & types\n",
    "print(\"\\nColumn dtypes (trimmed):\")\n",
    "print(df[numeric_cols + categorical_cols + [target_col]].dtypes)\n",
    "\n",
    "# 6) (Optional) save a tiny sample of the imputed data for your records/screenshots\n",
    "preview = df[numeric_cols + categorical_cols + [target_col]].head(12)\n",
    "preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24485439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CLEAN_PATH)\n",
    "\n",
    "numeric_cols = [\n",
    "    \"views\",\"likes\",\"comments\",\n",
    "    \"watch_time_minutes\",\"video_length_minutes\",\n",
    "    \"subscribers\",\"engagement_rate\",\n",
    "]\n",
    "categorical_cols = [\"category\",\"device\",\"country\"]\n",
    "target_col = \"ad_revenue_usd\"\n",
    "\n",
    "# Impute\n",
    "for c in numeric_cols:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "for c in categorical_cols:\n",
    "    df[c] = df[c].fillna(df[c].mode()[0])\n",
    "\n",
    "# Sanity check\n",
    "null_pct_after = (df.isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "print(\"=== Imputation done ===\")\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "print(\"Remaining NaNs (top 10):\\n\", null_pct_after.head(10))\n",
    "print(\"\\nDtypes:\\n\", df[numeric_cols + categorical_cols + [target_col]].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3 (Patched): split + preprocessing with imputers ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 0) Use YOUR cleaned file path (Windows-safe raw string)\n",
    "CLEAN_PATH = r\"C:\\Users\\WELCOME\\Desktop\\project 3\\youtube_clean.csv\"\n",
    "\n",
    "# (optional) derive folders for tomorrow’s export\n",
    "PROJ_DIR  = os.path.dirname(CLEAN_PATH)\n",
    "APP_DIR   = os.path.join(PROJ_DIR, \"app\")\n",
    "MODEL_PATH = os.path.join(APP_DIR, \"model.pkl\")\n",
    "\n",
    "# 1) Load cleaned snapshot from Day 1\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "\n",
    "# 2) Feature groups and target\n",
    "numeric_cols = [\n",
    "    \"views\",\"likes\",\"comments\",\n",
    "    \"watch_time_minutes\",\"video_length_minutes\",\n",
    "    \"subscribers\",\"engagement_rate\",\n",
    "]\n",
    "categorical_cols = [\"category\",\"device\",\"country\"]\n",
    "target_col = \"ad_revenue_usd\"\n",
    "\n",
    "X = df[numeric_cols + categorical_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# 3) Split (80/20, reproducible)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# 4) Build preprocessors (impute → scale / encode)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "# OneHotEncoder param differs across sklearn versions; handle both\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)  # sklearn >= 1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)         # older versions\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# 5) Fit preprocessor on TRAIN ONLY (prevents leakage)\n",
    "preprocess.fit(X_train)\n",
    "\n",
    "# 6) Transform to confirm shapes\n",
    "X_train_proc = preprocess.transform(X_train)\n",
    "X_test_proc  = preprocess.transform(X_test)\n",
    "\n",
    "# 7) Diagnostics (nice for viva)\n",
    "try:\n",
    "    cat_feature_names = preprocess.named_transformers_[\"cat\"].named_steps[\"ohe\"].get_feature_names_out(categorical_cols)\n",
    "except Exception:\n",
    "    cat_feature_names = preprocess.named_transformers_[\"cat\"].named_steps[\"ohe\"].get_feature_names(categorical_cols)\n",
    "\n",
    "print(\"=== Preprocessing fitted with imputers ===\")\n",
    "print(\"Train raw shape:\", X_train.shape, \" -> transformed:\", X_train_proc.shape)\n",
    "print(\"Test  raw shape:\", X_test.shape,  \" -> transformed:\", X_test_proc.shape)\n",
    "print(f\"Numeric cols: {len(numeric_cols)} | One-hot cols: {len(cat_feature_names)} | Total transformed cols: {X_train_proc.shape[1]}\")\n",
    "\n",
    "for col, cats in zip(categorical_cols, preprocess.named_transformers_[\"cat\"].named_steps[\"ohe\"].categories_):\n",
    "    print(f\"- {col}: {len(cats)} levels (e.g., {list(cats)[:5]}...)\")\n",
    "\n",
    "# Keep X_train, X_test, y_train, y_test, preprocess, MODEL_PATH in memory for next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJ_DIR = Path(r\"C:\\Users\\WELCOME\\Desktop\\project 3\")  # your project folder\n",
    "PROJ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_path = PROJ_DIR / \"model_comparison_results.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "print(\"Saved results to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# --- paths ---\n",
    "CLEAN_PATH = r\"C:\\Users\\WELCOME\\Desktop\\project 3\\youtube_clean.csv\"\n",
    "PROJ_DIR   = Path(r\"C:\\Users\\WELCOME\\Desktop\\project 3\")\n",
    "APP_DIR    = PROJ_DIR / \"app\"\n",
    "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = APP_DIR / \"model.pkl\"\n",
    "\n",
    "# --- reload full data (lets us fit on 100%) ---\n",
    "df_full = pd.read_csv(CLEAN_PATH)\n",
    "\n",
    "numeric_cols = [\n",
    "    \"views\",\"likes\",\"comments\",\n",
    "    \"watch_time_minutes\",\"video_length_minutes\",\n",
    "    \"subscribers\",\"engagement_rate\",\n",
    "]\n",
    "categorical_cols = [\"category\",\"device\",\"country\"]\n",
    "target_col = \"ad_revenue_usd\"\n",
    "\n",
    "X_full = df_full[numeric_cols + categorical_cols]\n",
    "y_full = df_full[target_col]\n",
    "\n",
    "# --- rebuild the same model dict and pick the winner from your results ---\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "best_name = results_df.iloc[0][\"model\"]  # from your Step 4 table\n",
    "best_model = models[best_name]\n",
    "\n",
    "# `preprocess` is from Step 3 (with imputers + scaler + OHE). Reuse it.\n",
    "final_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", best_model)])\n",
    "final_pipe.fit(X_full, y_full)\n",
    "\n",
    "dump(final_pipe, MODEL_PATH)\n",
    "print(\"Best model:\", best_name)\n",
    "print(\"Saved model to:\", MODEL_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
